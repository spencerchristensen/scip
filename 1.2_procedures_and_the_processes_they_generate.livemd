<!-- livebook:{"file_entries":[{"name":"fib.svg","type":"attachment"}]} -->

# SCIP 1.2 Procedures and the Processes They Generate

```elixir
Mix.install([])
```

## 1.2 Procedures and the Processes They Generate

The ability to visualize the processes (consequences) generated by procedures is critical to being an expert programmer.

A procedure is a pattern of the *local evolution* of a process, specifying how each stage is built one upon the other.

## 1.2.1 Linear Recursion and Iteration

It is important to observe the *shape* of a process to understand it.

We must not confuse the notion of *process* with the notion of a *procedure*. Let's look at two types of processes:

**Linear Recursive Process**: A process that chains together deferred evaluation until N recursive steps are reached.

*a recursive procedure*

```scheme
(define (factorial n)
  (if (= n 1)
        1
        (* n (factorial (- n 1)))))
```

*a linear recursive process*

```scheme
(factorial 6)
(* 6 (factorial 5))
(* 6 (* 5 (factorial 4)))
(* 6 (* 5 (* 4 (factorial 3))))
(* 6 (* 5 (* 4 (* 3 (factorial 2)))))
(* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 1)))))
(* 6 (* 5 (* 4 (* 3 2))))
(* 6 (* 5 (* 4 6)))
(* 6 (* 5 24))
(* 6 120)
720
```

**Iterative Process**: a process in which the state is maintained according to a fixed set of *state variables*, logic for how the state is updated, and an optional end test that dictates when the process should terminate.

*a recursive procedure*

```scheme
(define (factorial n)
  (fact-iter 1 1 n))

(define (fact-iter product counter max-count)
        (if (> counter max-count)
    product
    (fact-iter (* counter product)
               (+ counter 1)
               max-count)))
```

*a linear iterative process*

```scheme
(factorial 6)
(fact-iter   1 1 6)
(fact-iter   1 2 6)
(fact-iter   2 3 6)
(fact-iter   6 4 6)
(fact-iter  24 5 6)
(fact-iter 120 6 6)
(fact-iter 720 7 6)
720
```

We can describe both of these as recursive procedures in that the procedure calls the procedure itself. However such a statement informs us only of the syntactic fact of how the procedure is written, not how the procedure's process evolves.

Many languages are designed so that the interpretation of any recursive procedure results in an amount of memory that grows with the number of procedure calls, even if the procedure is in principle iterative. These languages will introduce looping constructs in order to support iterative processes: `for`, `while`, etc.

If an interpreter is able to execute an iterative process in constant space, even if defined by a recursive procedure, it is termed *tail-recursive*.

## 1.2.2 Tree Recursion

Consider the following procedure:

```scheme
(define (fib n)
  (cond ((= n 0) 0)
        ((= n 1) 1)
        (else (+ (fib (- n 1))
                 (fib (- n 2))))))
```

Let us observe what happens when we call `(fib 5)`

![](files/fib.svg)

* the process looks like a tree
* the branches split into two at each level
* the entire computation of `(fib 3)` is duplicated

Note that each vertical step is of the same size, whereas the horizontal steps expand exponentially.

The number of steps required by a tree-recursive process will be proportional to the number of nodes in the tree, while the space required will be proportional to the maximum depth of the tree.

The Fibonacci sequence could be rewritten to use an iterative process:

```scheme
(define (fib n)
  (fib-iter 1 0 n))

(define (fib-iter a b count)
  (if (= count 0)
      b
      (fib-iter (+ a b) a (- count 1))))
```

While the linear recursive process is exponential in its growth, it is easier to understand. One can observe that a recursive process could be recast as an iterative one with three state variables.

## 1.2.3 Orders of Growth

Processes differ considerably in the rates at which they consume computational resources. An *order of growth* can be used to obtain a gross measure of the resources required by a process as the inputs become larger.

$n$ = the size of the problem.

$R(n)$ = the amount of resources the process requires for a problem of size $n$.

$R(n) = \Theta(f(n))$ if there are positive constants $k^1$ and $k^2$ independent of $n$ such that:

$k1 f(n) \leq R(n) \leq k2 f(n)$

($\Theta(f(n))$ is pronounced "theta of $f(n)$")

Some common orders of growth:

$\Theta(n)$ grows linearly.

$\Theta(1)$ is constant.

$\Theta(n^2)$ is quadratic.

$\Theta(log\ n)$ is logarithmic.

## 1.2.4 Exponentiation

A *linear recursive process* requires $\Theta(n)$ steps and $\Theta(n)$ space.

A *linear iterative process* requires $\Theta(n)$ steps and $\Theta(1)$ space.

However, there are equations like exponentiation where we do not have to proceed incrementally but can instead compute it using a combination of specific calculations.

Let's say we want to solve $b^8$

Rather than:

$b \cdot (b \cdot (b \cdot (b \cdot (b \cdot (b \cdot (b \cdot b))))))$

We could:

$b^2 = b \cdot b$

$b^4 = b^2 \cdot b^2$

$b^8 = b^4 \cdot b^4$

This can be expressed as:

```scheme
(define (fast-expt b n)
  cond ((= n 0) 1)
       ((even? n) (square (fast-expt b (/ n 2))))
       (else (* b (fast-expt b (- n 1)))))
```

This would require $\Theta(log\ n)$ steps and $\Theta(log\ n)$ space.

## 1.2.6 (skipping 5): Probabilistic Method

There is a class of algorithms that while correctness is not guaranteed, the chance of error becomes arbitrarily small and thus allows the confident use of them. These are *probabilistic algorithms*.

